## Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering

Jonas Oppenlaender 1 , Rhema Linder 2 , and Johanna Silvennoinen 3

1 Independent Researcher, oppenlaenderj@acm.org

- 2 University of Tennessee, Knoxville, rlinder@utk.edu

3 University of Jyvaskyla, johanna.silvennoinen@jyu.fi

## Abstract

We are witnessing a novel era of creativity where anyone can create digital content via prompt-based learning (known as prompt engineering). This paper investigates prompt engineering as a novel creative skill for creating AI art with text-to-image generation. In three consecutive studies, we explore whether crowdsourced participants can 1) discern prompt quality, 2) write prompts, and 3) refine prompts. We find that participants could evaluate prompt quality and crafted descriptive prompts, but they lacked style-specific vocabulary necessary for effective prompting. This is in line with our hypothesis that prompt engineering is a new type of skill that is non-intuitive and must first be acquired (e.g., through means of practice and learning) before it can be used. Our studies deepen our understanding of prompt engineering and chart future research directions. We conclude by envisioning four potential futures for prompt engineering.

## 1 Introduction

We are entering an era in which anybody can generate digital images from text - a democratization of art and creative production. In this novel creative era, humans work within a human-computer co-creative framework (Kantosalo, Thattai Ravikumar, Grace, & Takala, 2020). Emerging digital technologies will co-evolve with humans in this digital revolution, which requires the renewal of human capabilities and competences (Haddington et al., 2021; H. Onan Demirel & Sha, 2024) and a human-centered design process (Grassini & Koivisto, 2024). One increasingly important human skill in this context is prompting due to it providing an intuitive language-based interface to artificial intelligence (AI). Prompting (or 'prompt engineering') is the skill and practice of writing inputs ('prompts') for generative models (Liu & Chilton, 2022; Oppenlaender, 2022). Prompt engineering is iterative and interactive - a dialogue between humans

and AI in an act of co-creation. As generative models become more widespread, prompt engineering has become an important research area on how humans interact with AI (Bach et al., 2022; Dang, Goller, Lehmann, & Buschek, 2023; Dang, Mecke, Lehmann, Goller, & Buschek, 2022; Deckers et al., 2023; Hou, Dong, Wang, Li, & Che, 2022; Jeong Soo Kim & Baek, 2024; Jiang et al., 2022; Liu & Chilton, 2022; Oppenlaender, Silvennoinen, Paananen, & Visuri, 2023; Reynolds & McDonell, 2021).

One area where prompt engineering has been particularly useful is the field of digital visual art. Image generation is often utilised in the later stage of the creation process in generating solutions, not in the ideation phase (Lee, Law, & Hoffman, 2024). State-of-the-art image generation systems, such as OpenAI's DALL-E (Ramesh, Dhariwal, Nichol, Chu, & Chen, 2022), Midjourney (Midjourney, 2022), and Stable Diffusion (Rombach, Blattmann, Lorenz, Esser, & Ommer, 2022), have been trained on large collections of text and images collected from the World Wide Web. These systems can synthesize high-quality images in a wide range of artistic styles from textual input prompts (Gabha, 2022; Liu & Chilton, 2022; Oppenlaender, 2022, 2023). Practitioners of text-to-image generation often use prompt engineering to improve the quality of their digital artworks (Liu & Chilton, 2022). Within the community of practitioners, certain keywords and phrases have been identified that act as 'prompt modifiers' (Oppenlaender, 2023). These keywords can, if included in a prompt, improve the quality of the generative model's output or make images appear in a specific artistic style (Gabha, 2022; Liu & Chilton, 2022; Oppenlaender, 2022). While a short prompt may already produce impressive results with these generative systems, the use of prompt modifiers can help practitioners unlock the systems' full potential (Oppenlaender, 2022, 2023; Xie, Pan, Ma, Jie, & Mei, 2023). The skillful application of prompt modifiers can distinguish expert practitioners of text-to-image generation from novices.

However, how people interact with image-generation systems is a relatively unexplored phenomenon (Kim, Eun, Oh, & Lee, 2024). In addition, whether prompt engineering is an intuitive skill or whether this skill must be acquired (e.g., through means of practice and iterative learning) has, so far, not been investigated. Investigating the skill of prompt engineering is important for several reasons.

For the field of AI art it is important to inform the development of future image generation systems and interfaces. A look at Stable Diffusion's Discord channel 1 provides anecdotal evidence that some prompts and keywords combinations circulating in the community of practitioners are not intuitive. Such keywords include, for instance, the modifier 'by Greg Rutkowski' or other popular keywords, such as 'smooth,' 'elegant,' 'luxury,' 'octane render,' ' trending on ,' and 'artstation' . These keywords are often used in combination with each other to boost the quality of generated images (Oppenlaender, 2023), resulting in unintuitive keyword combinations that a human user would likely never have chosen to describe the image to another human. With these many keywords comes a loss of control over the outcome. There is a high randomness to the outcome of text-to-image generation, and controlling the image generation (without resorting to additional tools, such as ControlNet (Zhang, Rao, & Agrawala, 2023), is difficult, even for

experts in prompt engineering). Keywords and modifiers are commonly applied by practitioners in the AI art community, but may confront laypeople with challenges of understanding the effect of modifiers on the resulting image. Further confounding the problem is that keywords in prompts can affect both the subject and style of a generated image simultaneously.

For the field of education, investigating the skill of prompt engineering is important, since the popularity and importance of the practice of prompt engineering is growing. Prompt engineering has been proven effective in solving difficult problems that were previously too complex to solve. This property makes prompt engineering useful in industry contexts. Therefore, a legitimate question to ask if whether prompt engineering should be included as a subject in school and higher education curricula. To inform this decision, one needs to know more about prompt engineering as a skill - what is its learning curve, how many hours would it take to learn, or is it even a skill? If people can just simply apply this language-based skill without much learning is important information to have in this context.

Furthermore, whether prompt engineering is a skill that humans apply intuitively or whether it is a new type of skill that needs to be acquired is important not only for the field of AI art and education, but also for research on human-AI interaction and the future of work in general. Consider, for instance, the sector of (higher) education and its future curricula. Agents based on language models (LMs) based have become vastly popular and promise to solve complex problems that previously required software engineering expertise (Wu et al., 2024). Such LM-based agents heavily rely on prompt engineering. The question, then, is whether current educational curricula should be extended with teaching on prompt engineering. However, if prompt engineering was a skill that everyone with sufficient knowledge of the English language could just apply without having to learn this skill, it would mean that extending the curricula with prompt engineering would not be necessary. An extension would take away from the time to teach other, more important, skills. On the other hand, if there is a learning curve to it, or if the skill proves to be transferable enough to be valuable, then its adoption in curricula would be warranted.

A source for unintuitiveness of the current practice of prompt engineering is a potential misalignment between the human-written prompts and the way in which text-to-image models interpret prompts. Compared to how we humans understand a prompt and its constituents, text-to-image generative models may attach very different meanings to some keywords in the prompt. Further, many AI-generated images are shared on social media, often with stunning results. However, if what we see on social media is the result of the application of prompt engineering by skilled experts, then the generative content that we encounter on social media could be skewed by a small group of highly skilled practitioners. Or from another perspective, if prompt engineering is an acquired skill that requires expertise and training, this could give rise to novel creative professions with implications for the future of work. On the other hand, we run the risk of assigning too much importance to prompting as a method for interacting with generative models if prompt engineering is an innate ability and an intuitive artistic skill that is acquired quickly (Oppenlaender, Silvennoinen, et al., 2023; Oppenlaender, Visuri, Paananen, Linder, & Silvennoinen, 2023).

In this paper, we explore the creative skill of prompt engineering in three studies with a total of 227 participants recruited from Amazon Mechanical Turk (MTurk), a crowdsourcing platform. The timing of our three studies is significant (see Table 1. The studies were conducted at a time when text-to-image generation was still relatively unknown. This allowed us to study whether participants could apply prompt engineering intuitively, or whether prompt engineering is a skill that must be learned through many iterations. We expand on the timing of this study in Section 6.3.

Table 1: Overview of the three studies presented in this paper.

|   No. |   Participants | Study date       | Study purpose                                          | Research question                                                                                                         |
|-------|----------------|------------------|--------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|
|     1 |             52 | May 19-23, 2022  | Test participants' understanding of prompt quality     | Are participants able to tell the quality of an image from the tex- tual input prompt? Can participants effectively write |
|     2 |            125 | June 12-23, 2022 | Test participants' ability to write prompts            | prompts to create digital art- works?                                                                                     |
|     3 |             50 | June 20-27, 2022 | Test participants' ability to revise their own prompts | Can participants improve their prompts to generate better digi- tal artworks?                                             |

In Study 1 , we explore participants' understanding of how a text-to-image generation system produces images of varying quality depending on the phrasing of input prompts. A feeling of what contributes to the quality of a prompt could enable participants to write prompts and create high-quality images. In our within-subject experiment, participants separately rated the aesthetic appeal of textual prompts and matching images generated with a text-to-image generation system. We hypothesize that a high degree of consistency within the participants' two ratings may point toward there being a strong understanding of what makes a 'good' prompt.

Key insights from Study 1: We find participants are able to grasp what makes a 'good' prompt. Being able to discern good from bad prompts would, in theory, allow participants to write effective prompts.

In Study 2 , we test the insights from the above two studies in practice. We invite participants to apply their knowledge and expertise by writing three input prompts for a text-to-image generation system with the specific aim of creating a digital artwork (and without seeing the generated images). We analyze participants' use of descriptive language and the use of prompt modifiers that could influence the quality and style of the resulting artworks. In Study 3 , we then invite the same participants who participated in the Study 2 to review the images generated from their own prompts. Each participant was asked to improve their prompts with the specific task of creating an artwork of high visual quality. Our hypothesis is that if prompt engineering is an intuitive skill innate to humans, participants will be able to apply it immediately. On the other hand, if participants are not able to significantly improve their images due to few interactions with the text-to-image generation system within our studies, then this may indicate that the skill of prompt engineering needs to be acquired before it can be applied in practice.

Key insights from studies 2 and 3: We find that while participants were able to describe artworks in rich descriptive language, almost none of the participants used specific keywords to adapt the style of their artworks or modify the images in other ways. Moreover, participants were not able to significantly improve the quality of the artworks in the follow-up study. This points to prompt engineering being a non-intuitive skill that people first need to acquire before it can be applied in meaningful ways.

In summary, our three studies find that while laypeople participants had the prerequisites to write prompts for AI art and were good at crafting descriptive prompts, they lacked style-specific vocabulary necessary for effective prompt engineering. We conclude by speculating on four potential futures for prompt engineering.

## 2 Related Work

## 2.1 Text-to-Image Generation with Deep Learning

Text-to-image generation is a type of generative deep learning technology that allows users to create images from text descriptions. This technology has gained significant interest since early 2021, when OpenAI published the results of DALL-E (Ramesh et al., 2021) and the weights of their CLIP model (Radford et al., 2021). CLIP is a multi-modal model trained on over 400 million text and image pairs from the Web. The model can be used in text-to-image generation systems to guide the generation of high-fidelity images. Many approaches and architectures for image generation with deep learning have since been developed, such as diffusion models (Dhariwal & Nichol, 2021). These approaches typically use machine learning models trained with contrastive language-image techniques using training data scraped from the Web. These systems are text-conditional, meaning they use text as input for image synthesis. This input, known as 'prompt,' describes the image to the system, which then generates one or more images without further input.

## 2.2 Prompt Engineering for AI Art

The practice of crafting input prompts is referred to as prompt engineering (or prompting for short). In this section, we explain the 'engineering' character of prompt engineering and how prompt engineering is applied for generating AI art.

## 2.2.1 The engineering character of prompting

The term prompt engineering was originally coined by Gwern Branwen in the context of writing textual inputs for OpenAI's GPT-3 language model (Liu & Chilton, 2022). 'Engineering,' in this case, does not refer to a hard science as found in science, technology, engineering, and mathematics (STEM) disciplines. Prompt engineering is a term that originates from within the online community of practitioners. Practitioners include artists and creative professionals, but also novices, amateurs, and more serious 'Pro-Ams' (Hoare, Benford, Jones, & Milic-Frayling, 2014) aiming, for instance, to sell their creations as digital art based on

non-fungible tokens (NFTs) (Kugler, 2021). Not every member of this online community may identify as a prompt engineer. An alternative self-understanding could be 'promptist' (Hayward, 2022) or 'AI artist' (Zylinska, 2020). One aspect of prompt engineering that relates to its engineering character is that it often involves systematic experimentation through trial and error (Liu & Chilton, 2022). The challenge for the prompt engineer is not only to find the right terms to describe an intended output and the right m, but also to anticipate how other people would have described and reacted to the output on the World Wide Web.

## 2.2.2 Prompt engineering for creating AI art

Art generated by artificial intelligence, or 'AI art' (Zylinska, 2020), has become a popular application for prompt engineering (Oppenlaender, 2022). An online community around AI art has formed, sharing images and prompts on various platforms. Within this community, certain practices for writing prompts have emerged. For example, prompts often follow a specific pattern, such as the following template (Smith, 2022):

[Medium] [Subject] [Artist(s)] [Details] [Image repository support]

A typical prompt could be (Allen, 2022):

A beautiful painting of a singular lighthouse, shining its light across a tumultuous sea of blood by greg rutkowski and thomas kinkade, trending on artstation.

Prompt modifiers, such as the underlined terms above, are added to a prompt to influence the resulting image in a specific way (Liu & Chilton, 2022; Oppenlaender, 2022, 2023). Prompt modifiers are an important technique in prompt engineering for AI art because they allow the prompt engineer to control the output of the text-to-image generation system. Prompt modifiers may make the resulting images subjectively more aesthetic and attractive (Liu & Chilton, 2022; Oppenlaender, 2022).

Different types of prompt modifiers are used in the AI art community (Oppenlaender, 2023), but the two most common types of modifiers affect the style and quality of images. These prompt modifiers consist of specific keywords and phrases that have been found to modify the style or quality of an image (or both). Modifiers that affect the quality of images can be referred to as 'quality boosters' (Oppenlaender, 2023), and include phrases such as 'trending on artstation,' 'unreal engine,' 'CGSociety,' '8k,' and 'postprocessing.' Style modifiers affect the style of an image and can include a wide variety of open domain keywords and phrases, such as 'oil painting,' 'in the style of surrealism,' or 'by James Gurney' (Oppenlaender, 2023).

Human-centered research on prompt engineering for text-to-image synthesis in the field of HumanComputer Interaction (HCI) is still in its early stages. The study by Liu and Chilton (2022) on subject and style keywords in textual input prompts mentioned that without knowledge of prompt modifiers, users must engage in 'brute-force trial and error'. The authors presented design guidelines to help people produce better results with text-to-image generative models (Liu & Chilton, 2022). Qiao, Liu, and Chilton (2022) conducted an experiment on using images as visual input prompts, resulting in design guidelines for improving subject representations in AI art. Besides these guidelines, there are also many community-created

resources that offer guidance for novices and practitioners of AI art, such as the 'Traveler's Guide to the Latent Space' by Smith (2022), Zippy's 'Disco Diffusion Cheatsheet' (Allen, 2022), and the 'Disco Diffusion Artist Studies' by Gabha (2022). These resources provide a wealth of information about prompt modifiers for producing high-quality visual artifacts.

## 2.3 Prompt Engineering as a Skill

Merriam-Webster defines 'skill' as 'the ability to use one's knowledge effectively and readily in execution or performance' and 'a learned power of doing something competently: a developed aptitude or ability' (Merriam-Webster, 2024). This definition captures the essence of skill as not only having knowledge, but also the capability to apply the knowledge effectively in practical situations and in real-world contexts.

In our work, we define 'skill in prompt engineering' as the ability to effectively utilize language and prior knowledge to craft prompts that guide generative models towards desired outputs. This encompasses not only the basic knowledge of the relevant language syntax but also the strategic use of prompt modifiers elements that refine or alter the direction of generative outputs. Our study aims to investigate whether individuals, particularly participants recruited from a crowdsourcing platform, possess the necessary foundational knowledge to write effective prompts. More importantly, we assess if these participants can translate this knowledge into practical application, demonstrating a skilled use of prompt modifiers in the context of text-to-image generation for AI art. The inability to effectively apply knowledge in practice may suggest a lack of skill, underscoring the need for knowledge acquisition and refinement through learning and training. This approach aligns with our objective to elucidate whether prompt engineering is an innate ability or whether it must be acquired (e.g., through practice and learning).

## 2.4 Prior Work on Applying Skill in Practice

Our research is related to prior work focusing on how individuals acquire skills and how they apply them in practice. In particular, learning how to use web search is a related area of work. This section is structured around three main themes identified in the literature: the divergence between search and domain expertise, the strategies involved in how people rewrite a query after a failed attempt, and teaching how to improve query authorship.

## Divergence Between Search and Domain Expertise

The first theme addresses the relationship between domain expertise and search expertise. White, Dumais, and Teevan (2009) provide an insightful analysis of how domain knowledge influences web search behavior. Their work demonstrates that domain experts and novices exhibit markedly different search strategies and outcomes. This finding is supported and extended by Wood et al. (2016), who explore the relative contributions of domain knowledge and search expertise in conducting effective internet searches (Huang &

Efthimiadis, 2009). These studies underscore the distinct nature of search expertise, separate from domainspecific knowledge, highlighting its importance in effective information retrieval.

## Query Reformulation Strategies

The second theme revolves around how individuals modify their search queries following unsuccessful attempts. Huang and Efthimiadis (2009) offer a comprehensive examination of query reformulation strategies in web search logs. Their analysis reveals the common patterns and tactics users employ when their initial search queries fail to yield desired results. This research is crucial in understanding the adaptive behaviors of users in response to the challenges they encounter during web searches.

## Educational Approaches to Query Authorship

The third theme relates to methods for teaching effective query formulation. Bateman, Teevan, and White (2012) contribute significantly to this area through their development of the Search Dashboard tool. Their study examines how tools that facilitate reflection and comparison can impact users' search behaviors, leading to more effective search strategies. This work is particularly relevant for designing educational interventions and tools aimed at enhancing the search skills of users.

Our studies draw upon and contribute to these existing bodies of work. We extend the understanding of how individuals apply their knowledge in writing prompts, and how they reformulate a prompt in an attempt to improve upon a first prompt. Both are crucial aspects of information literacy in the digital age.

## 3 Study 1: Understanding Prompt Engineering

We conducted a within-subject experiment to study participants' understanding of prompt engineering. Participants were asked to rate the textual prompts and the corresponding AI-generated images. We hypothesize that participants with a strong understanding of prompt engineering would exhibit a high consistency between the ratings in the two modalities. In other words, if someone can predict the aesthetic appeal of an image from its textual prompt, they likely have a good sense of how prompt engineering works. The study design reflects the knowledge that prompt engineers would draw on in practice: first write a prompt with the intention to produce a high quality image, then observe and assess the quality of the resulting image. A good understanding of textual prompts is crucial for predicting how well a prompt will perform. For instance, prompts incorporating rich descriptive language and multiple prompt modifiers tend to yield artworks of higher quality compared to prompts lacking these attributes. 2 In the following section, we describe the study design in detail.

## 3.1 Method

## 3.1.1 Research materials

We curated a set of prompts and images created with Midjourney, a text-to-image generation system and community of AI art practitioners. Using purposeful sampling, we selected 111 images from the corpus of thousands of Midjourney images generated by the first author. Our choice to use the author's image corpus has several advantages. The corpus includes images with a range of different prompt modifiers commonly used on Midjourney and we avoid intruding on others' intellectual property rights. Further, the author has experience with text-to-image generation and can distinguish failed attempts from successful ones. This allowed us to purposefully sample images with varying levels of subjective quality. Specifically, we selected 59 images judged as failed attempts and 52 images of high aesthetic quality. We kept the format of four images per prompt, as it resembles the output a prompt engineer would typically receive on Midjourney.

To assess the aesthetic quality of the 111 images in the dataset, we recruited ten volunteer raters from two academic institutions. The raters had diverse backgrounds in Computer Science, Information Sciences, Human-Computer Interaction, Cognitive Science, Electrical Engineering, and Design. They consisted of 2 Professors, 3 PostDocs, 3 PhD students, 1 Master student, and 1 project engineer (5 men and 5 women, age range 24-48 years). Raters completed a simple binary classification task to classify the images as high or low quality based on their aesthetic appeal. Raters were informed that there was an unequal number of images in each category. The inter-rater agreement over all images, as measured by Fleiss' kappa, was fair, κ = 0 . 34, z = 23 . 9, p < 0 . 00, 95% CI [0.31, 0.37].We discussed the ratings and selected images for further study. Only images with perfect agreement among the ten raters were selected for further study. From the set with perfect agreement among raters, we selected ten high- and ten low-quality images.

We contend that high-quality images can be determined by aesthetic quality ratings from 10 volunteers, without disclosing the prompts to them, for the following reasons. Classifying images with high and low aesthetics is a relatively easy task (Kong, Shen, Lin, Mech, & Fowlkes, 2016). The evaluation of 'high-quality' and 'low-quality' images by a limited number of volunteers is not aimed at establishing a universal standard of image quality. We contend that understanding the subjective quality perception of AI-generated images is as important as their fidelity to the prompts. While we recognize the importance of the alignment between the image and its corresponding prompt in determining quality, our primary focus in this study is on the aesthetic appeal as perceived by individuals without the context of the prompts. Inspired by the 'wisdom of the crowd' (Surowiecki, 2005), our objective is to leverage perceptual differences in aesthetic appreciation among multiple individuals in order to devise two distinct sets of images, without the influence of the original prompts used to create the images. Future studies could integrate prompt fidelity as an additional dimension of image quality.

The final set contains 20 images and respective prompts of varying quality (see Figure 1 and Appendix A).

Figure 1: Exemplars of images used in Study 2. The full set of images and prompts is listed in Appendix A.

<!-- image -->

<!-- image -->

1a) High aesthetic appeal

<!-- image -->

1b) Low aesthetic appeal

<!-- image -->

## 3.1.2 Study design

We conducted a within-subject experiment with two distinct conditions. The first condition required participants to rate 20 AI-generated images on a 5-point Absolute Category Rating (ACR) scale (Pinson et al., 2012; Siahaan, Redi, & Hanjalic, 2014) (refer to Figure 2a). The ACR is an established scale for producing reliable judgments (Siahaan et al., 2014), noted for its insensitivity to variables such as lighting, monitor calibration, language, and country (Pinson et al., 2012). In the second condition, participants were presented with 20 textual prompts and asked to imagine and rate the images they believed these prompts would generate, using the same ACR scale. Here, participants were shown only the prompts, not the actual images. Each prompt was introduced with 'Imagine the image generated from the prompt: . . . ' accompanied by a descriptive task reminder (see Figure 2b).

<!-- image -->

Figure 2: Example of items used in Study 1 for rating the aesthetic appeal of a) AI-generated artworks and b) prompts. The latter task was prefixed by an instruction to 'imagine the image generated from the prompt.'

<!-- image -->

The instructions were carefully designed to avoid confounding factors. For instance, we used neutral wording and avoided referring to the images as artworks to prevent higher positive aesthetic ratings (Arai & Kawabata, 2016; Gerger, Leder, & Kremer, 2014; Pelowski, Gerger, Chetouani, Markey, & Leder, 2017; Van Dongen, Van Strien, & Dijkstra, 2016). Note that our aim was not to measure exact ground-truth ratings for aesthetic appeal, but to study differences in ratings within participants in a within-subject design.

After the two conditions, we collected basic demographics including participants' self-rated interest in art and experience with practicing art. We also included an optional open-ended item for participants

to elaborate on their experience with text-to-image generation. Experience with art and text-to-image generation were measured on 5-point Likert scales, and self-rated experience with art was measured as a binary variable.

## 3.1.3 Participant recruitment and procedure

We recruited US-based participants from Amazon Mechanical Turk (MTurk) with a task approval rate greater than 95% and at least 1000 completed tasks. This combination of qualification criteria is common in crowdsourcing research (e.g., (Hope et al., 2022)). The experiment was implemented as a survey task and hosted on Google Forms. Participants were paid US $ 1.50 for completing the survey. The price was determined from the average completion times in a small-scale pilot study ( N = 9, US $ 1 per task) (Oppenlaender, Abbas, & Gadiraju, 2024).

The task consisted of 31 items in total, including a consent form, an introduction to the study, 20 ratings of prompts, 20 ratings of images, ten demographic items, and one consistency check. Participants underwent the two conditions (rating of prompt and rating of images) in balanced order. Half of the participants first rated the prompts, then the images, and the other half vice versa.

To prevent bias, we anonymized the filenames of the images and assigned a random numeric code to each image to make it harder to associate the images with the prompts from the previous survey section. As a check for consistency, we duplicated one image and collected a rating for this image (L1, see Appendix A.2). Participants who differed in their rating for this duplicated image by greater than one category on the ordinal ACR scale were excluded from analysis. We excluded four participants for failing this consistency check and another two participants for having completed the survey without completing the task on MTurk. The final sample included 52 participants.

## 3.1.4 Analysis

We hypothesize that participants can detect a relationship between the quality of a prompt, in terms of its ability to depict visual art through human imagination, and the quality of the visual artwork generated by the text-to-image generation system. To test this hypothesis, we performed a correlation test using Pearson's product-moment correlation to look at the relationship between paired scores for each type. We investigated the correlation between art experience and average error for each participant. Average error per participant was calculated by taking the average absolute difference between each pair of prompt and artwork rating. For example, if all prompts were rated as 2 and all artworks as 5, the average error would be 3.

## 3.2 Results

## 3.2.1 Participants

Participants ( N = 52) were between 24 and 67 years of age ( M = 38 . 2 years, SD = 12 . 98 years) and included 31 men and 21 women (no non-binary) from diverse educational backgrounds (27 Bachelor's degrees, 10 Master's degrees, among others). A sizable fraction of the participants (46%) reported having an educational background in the arts. Twenty-nine participants agreed and nine strongly agreed that they had visited many museums and art galleries ( M = 3 . 60, SD = 1 . 18). However, participants did not practice art often ( M = 3 . 08, SD = 1 . 28). Overall, participants were interested in AI generated art ( M = 3 . 69, SD = 0 . 83), but had little experience with text-to-image generation ( M = 2 . 58, SD = 1 . 43). Only three participants mentioned having used text-to-image generation (DALL-E mini/Craiyon) before.

## 3.2.2 Visual and prompt ratings

Our study design asked participants to rate both Prompts and Visual artwork. Since participants did not receive special training for prompt engineering or text-based AI art, our goal was to understand the quality of our participants' perceptions. We show the histogram of scores broken into groups for each Art Type (Prompt and Artwork) in Figure 3 and Table 2 and the Quality (high or low) as described previously. Visually, these show differences across groups, with the distributions of Artworks leaning towards higher quality than prompts.

We used a Kruskal-Wallis rank sum test on these four unique groups, finding a significant difference between the study conditions ( χ 2 = 231 . 4, p < 10 15 , df = 3). Following this significant result, we performed post-hoc Dunn's test pairwise across each group with Bonferroni correction for p-values. Each of these pairs had significant results with a p-value of less than 10 -4 , except for Artwork-High versus Prompt-High, in which p < . 004. This implies that the median values among all comparisons of groups (i.e. Artwork-High, Artwork-Low, Prompt-High, Prompt-Low) are significantly different from each other.

Participants were able to differentiate images with low visual aesthetic quality from high quality images. Artwork-High has a higher mean rating ( µ = 3 . 70) compared to Prompt-Low ( µ = 3 . 39). Likewise, participants were able to distinguish between high and low quality by imaging what would be produced based on textual prompts. Prompt-High has a higher mean rating ( µ = 3 . 87) compared to Prompt-Low ( µ = 2 . 78). The overall span between the Artwork High and Low is larger for Prompts (3 . 87 -2 . 78 = 1 . 09) than for Artworks (3 . 70 -3 . 39 = . 31). Both High and Low quality Artworks had distributions that favored a rating of 4, while Prompt-Low has a relatively flat distribution across values of 1 to 4 (see Figure 3).

## 3.2.3 Connection between visual image and prompt quality

While in theory, prompts that can help readers conjure (i.e. visualize or imagine) more aesthetically appealing mental images will also generate better Artwork, it is not clear whether this would be the case for

Figure 3: Histograms of rating scores provided by participants in the two conditions of the within-subject study. Scores by participants ( N = 52) for images (left) and prompts (right) with high aesthetic quality (top; H1-H10) and low aesthetic quality (bottom; L1-L10) in Study 2.

<!-- image -->

.

Table 2: Average rating across Art Type and Quality in Study 1

| Art Type   | Quality   |   Mean |   Std Dev |
|------------|-----------|--------|-----------|
| Artwork    | High      |   3.7  |      1.04 |
| Artwork    | Low       |   3.39 |      1.15 |
| Prompt     | High      |   3.87 |      1.07 |
| Prompt     | Low       |   2.78 |      1.28 |

crowdsourced participants. While our participants were not able to directly associate Prompts to Artworks, each Artwork had a matching Prompt. We used a Person's product-moment correlation test to measure whether ratings for the Prompt and Artwork are correlated. The test shows a weak ( r = . 29, 95% CI [.23, .34]) but significant ( p < 10 -15 ) positive correlation between ratings from Artworks and Prompts. This indicates that when a Prompt is seen as having a higher quality, that it is also more likely that the Artwork will appear as having a high quality.

## 4 Study 2: Writing Prompts

Our aim with this study is to probe whether laypeople recruited from a crowdsourcing platform have the ability to come up with effective input prompts for text-to-image generation systems with a specific focus on generating digital artworks. The presence of style modifiers in an input prompt may indicate an understanding of text-to-image generation and how effective prompts can be formulated.

## 4.1 Method

## 4.1.1 Study design

We designed a creative crowdsourcing task eliciting three textual prompts from each participant. The task included a short introduction and the following instructions:

Imagine an artificial intelligence that turns textual input prompts into digital artworks. Your task is to produce three artworks. To this end, you will write three different input prompts for the artificial intelligence. You should aim to maximize the visual attractiveness and aesthetic qualities of the digital artworks generated from your input prompts.

Participants were asked to make their artworks as visually attractive and high-quality as possible. We did not mention that prompt modifiers could be used in the prompt and wrote the instructions to avoid priming participants with a specific style (i.e., we told participants to produce 'artworks' rather than 'paintings'). Note, however, that we did not aim to precisely measure attractiveness and quality, but wanted participants to think about the overall visual and aesthetic quality of the images. Participants were told that there was no right or wrong answer, but tasks would be rejected if they didn't follow the instructions. To avoid influencing participants prompt modifications in our follow-up study (Study 3), we merely collected prompts from participants in this study - the outcome of the image generation was not shown to participants in this study.

As additional questions in the task, we asked whether the participant had experience with text-to-image generation and we collected basic demographics. Participants were paid US $ 0.16 per completed task. The pricing was estimated from the average task completion times in a pilot study ( N = 10, US $ 0.12 per task). In this pilot study, we noticed some participants wrote a series of consecutive instructions for the AI. The task design and instructions were subsequently adjusted to elicit complete prompts.

## 4.1.2 Participant recruitment

We recruited 137 unique participants recruited from Amazon Mechanical Turk using the same qualification criteria as in Study 1. Ten tasks had to be rejected due to clearly no attempt being made to answer the task with relevant information. The ten tasks were republished for other participants. After collecting the data, we manually reviewed the results and removed a further twelve responses from participants who obviously tried to game the task. The final set includes 375 prompts written by 125 unique participants (three prompts per participant).

## 4.1.3 Analysis

The analysis of the prompts was conducted with mixed methods. For each prompt, we qualitatively and quantitatively analyzed the prompts, as follows.

Prompt modifiers Our analysis focused on identifying the presence of specific keywords and phrases frequently used within the AI art community to influence the style and quality of AI-generated images (Oppenlaender, 2022, 2023). We opted for manual analysis in this case. An initial screening indicated that a very small portion of the prompts utilized prompt modifiers, making automated methods unnecessary for this specific task.

The analysis process involved the first author of this paper who systematically reviewed each prompt, with focus on identifying specific language patterns and stylistic phrases known to impact the AI's generative capabilities. This included looking for explicit instructions or adjectives that might alter the style or quality of the generated images. Given the clear and specific nature of these prompt modifiers, the coding focused on the presence or absence of these elements in each prompt. We determined that the coding process was straightforward - i.e., it did not require complex categorization or subjective interpretation - and, thus, did not necessitate validation via inter-rater agreement (McDonald, Schoenebeck, & Forte, 2019).

We analyzed whether the prompts contained certain keywords and phrases commonly used in the AI art community to modify the style and quality of AI generated images (Oppenlaender, 2022, 2023). We decided on manual analysis because a preliminary screening revealed that very few prompts contained prompt modifiers. Each prompt was analyzed by an author of this paper. We coded the presence of prompt modifiers and report on their nature and use. We did not calculate inter-rater agreement because the coding was straight-forward (McDonald et al., 2019).

Descriptive language A prompt written in descriptive language is likely to generate images of high quality. We quantitatively assessed whether the prompts contained descriptive language by calculating a number of statistical indices for each prompt:

- · The number of words (tokens) and unique words (types) in the prompt. In general, longer prompts are more likely to include certain keywords (whether on purpose or by accident) that may trigger the image generation system to generate images with high quality or in a certain style.
- · The Type-Token Ratio (TTR) (Johnson, 1944), a standard measure for lexical diversity defined as the number of types divided by the number of tokens in the prompt. 3 A token, in this case, is a discrete word whereas a type is a unique token in the prompt. For calculating the TTR, we used Kristopher Kyle's lexical-diversity Python package (Kyle, 2018).

## 4.2 Results

## 4.2.1 Participants

The 125 participants in our sample included 55 men, 67 women, 1 non-binary, and two participants who did not to disclose their gender identity. The age of participants ranged from 19 to 71 years ( M = 41 . 08 years, SD = 13 . 44 years). The majority of participants (98.40%) reported English being their first language. Thirty-seven participants (30.33%) responded positively to the question that they had 'experience with textbased image generation systems.' We had no explanation for this surprisingly high number at this point, but inquired more about the participants' background in our follow-up study in Section 5. Median completion times were higher than estimated in the pilot study, reaching 197 seconds. It is possible that completion times are skewed due to participants reserving tasks in bulk.

## 4.2.2 On the use of descriptive language

The prompts were of varying length, ranging from 1 to 134 tokens with an average of 12.54 tokens per prompt ( SD = 14 . 65 tokens). Overall, the length of prompts was appropriate for text-to-image generation with only four participants producing overly long prompts. On average, participants used 3.27 nouns to describe the subjects in their prompt ( SD = 3 . 36). Participants used verbs only sparingly in their prompts ( M = 0 . 36, SD = 1 . 02). The average number of prepositions ( M = 1 . 78, SD = 2 . 27) was higher than the average number of adjectives ( M = 1 . 65, SD = 1 . 95). However, this number is skewed by four participants who provided long prompts. These participants were very specific in what their images should contain, with many prepositions being used to denote the relative positions of subjects in the artwork ( Max = 21 prepositions per prompt).

Overall, participants used rich descriptive language. The participants were creative and often described beautiful natural scenery. The main topics in the participants' prompts were landscapes, sunsets, and animals. We note that the richness of the language in the prompts primarily is a result of the use of adjectives. On average, participants used 1.65 adjectives in their prompt ( SD = 1 . 95). Colors, in particular, were popular among participants to describe the subjects in their artworks. The following prompts exemplify the creativity and the use of descriptive language among participants:

- · beautiful landscape with majestic mountains and a bright blue lake
- · bright yellow sun against a blue sky with puffy clouds
- · A fruit bowl with vibrant colored fruits in it and a contrasting background
- · Dragon on the tower of a castle in a storm.
- · Knight holding a sword that shines in the sunlight
- · A white fluffy puppy is playing in the grass with a large blue ball that is twice his size.

- · A shiny black horse with eyes like coal run in a lush green grassy field
- · There should be a beautiful green forest, full of leaves, with dark brown earth beneath, and a girl in a dress sitting on the ground holding a book.

More than half of the prompts (58.13%) did not repeat any tokens (that is, they had a TTR of 1; M = 0 . 94, SD = 0 . 10). Most of the repetitions in prompts stem from the participants' need to identify the relative positions of subjects in the image (e.g., '[...] Touching the black line and going all the way across the top of the black line should be a dark green line. Above the dark green line should be a medium green line. [...]' ). Repetitions, as a stylistic element in prompts (Oppenlaender, 2023), were not being used. Only 27 prompts (7.2% of all prompts) contained cardinal numbers ( M = 0 . 07, SD = 0 . 29). Two cardinal numbers referred to a period in time which could potentially trigger the image generation system to produce images in a certain style.

Even though we tried to mitigate it in the task design and the instructions, we noticed 18 participants (14.4%) still provided direct instructions to the AI instead of prompts describing the image content. These participants either wrote three separate instructions to the AI (e.g., 'Generate a white 250 ml tea glass [...],' 'Draw three separate triangles [...],' and 'Show me some digital artwork from a brand new artist.' ) or they wrote three consecutive instructions as we had observed in our pilot study. The latter may not include nouns as subject terms and could thus result in images with an undetermined subject (e.g., 'sharpen image' ). Two participants thought they could chat with the AI, asking it, for instance, 'Which do you prefer: starry night sky or blue sea at dawn?,' 'Enter your favorite geometric shape,' and 'Can you paint me a rendition of the Monalisa?' .

## 4.2.3 On the use of prompt modifiers

Even though participants were specifically instructed to create a digital artwork, we found only very few participants included style information in their prompts. Many participants described a scene in rich descriptive language, but neither mentioned artistic styles, artist names, genres, art media, nor specific artistic techniques. The participants' prompts may have described an artwork, but without style information, the style of the generated image is left to chance and the resulting images may not match the participants' intent and expectations.

Overall, the prompts did not follow the prompt template mentioned in Section 2.2.2 and best practices common in the AI art community were not followed. Only one participant made purposeful use of a prompt modifier commonly used in the AI art community. This prompt modifier is 'unreal engine.' 4 The participant used this modifier in all her three prompts by concatenating it to the prompt with a plus sign, e.g. 'rainbow tyrannosaurus rex + unreal engine.' A small minority of participants used generic keywords that could trigger a specific style in text-to-image generation systems. For instance, the generic term 'artwork' was

used in 16 prompts (4.3%). The following list of examples reflects almost the entire set of prompts containing explicit style information among the 375 prompts written by participants (with style modifiers underlined):

- · Cubism portrait of a Labrador Retriever using reds and oranges
- · Paint a portrait of an old man in a park.
- · Draw a sketch of an airplane.
- · Abstract trippy colorful background
- · surreal sky castle
- · Can you paint me a rendition of the Monalisa?
- · Bob Ross, Claude Monet, Vincent Van Gogh
- · Are you able to produce any of rodans work.
- · what can you do, can you make pointillism artwork?

Besides this sparse - and sometimes accidental - addition of style information, we find that overall, participants did not control the style of their creations. Instead of prompt modifiers, the participants' artwork styles were mainly determined by the participants' use of descriptive language.

## 5 Study 3: Improving Prompts

In a follow-up study, we investigated whether participants could improve their artworks. This study aimed to answer the question of whether prompt engineering is an innate skill that we humans apply intuitively or whether it is an acquired skill that requires expertise and practice (e.g., via learning to write prompts from repeated interactions with the text-to-image generation system) and knowledge of certain keywords and key phrases (prompt modifiers), as discussed in Section 2.2.2 and Section 4.2.3. We hypothesize that if prompt engineering is a learned skill, participants will not be able to significantly improve their artworks after only one iteration.

## 5.1 Method

## 5.1.1 Study design

We invited the same participants who participated in Study 2 to review images generated from their own prompts. Participants were then asked to improve their three prompts. To this end, we designed a task that introduced the participant to the study's purpose, using the same instructions as in the previous study.

We additionally highlighted that if the images presented to the participant did not look like artworks, the prompt should be adjusted. Like in the previous study, we avoided to mention that prompt modifiers could be used to achieve this aim.

Participants were given five images for each of the three prompts they wrote in Study 2. We used the workerId variable on MTurk to load the participant's previous prompts and images. Participants were then asked to rewrite and improve their three prompts. The task included two input fields, one pre-filled with their previous prompt and one for optional negative terms. In practice, negative terms are an important part of the toolbox of prompt engineers, primarily used for controlling the subject and quality of the image generation (Oppenlaender, 2023). For example, defining 'watermark' and 'shutterstock' as negative terms can reduce the occurrence of text and watermarks in the resulting image. Given the task of improving their previously generated artworks, we introduced negative terms as a possible tool for making improvements in Study 3. We studied this by incorporating it into our study design. Participants were introduced to the potentially surprising effects of negative terms with the help of an example. The example explained that adding 'zebra' as a negative term to a prompt for a pedestrian crossing could potentially result in an image of a plain road (due to stripes being removed).

For each prompt, we also collected information on whether the images matched the participant's original expectations (given the previous prompt) and whether the participant thought the prompt needed improvement (both on a Likert-scale from 1 - Strongly Disagree to 5 - Strongly Agree). The latter was added to identify cases in which participants thought that no further improvement of the prompt was necessary. We also asked participants to rate their confidence that the new prompt would result in a better artwork (on a Likert scale from 1 - Not At All Confident to 5 - Highly Confident). The task concluded with demographic questions, including the participant's experience with text-based image generation and interest in viewing and practicing art. The task design was tested and improved in a small-scale pilot study ( N = 8; US $ 1 per task). The payment was set to US $ 1.75, aiming for an hourly pay of above minimum wage in the United States.

## 5.1.2 Research materials

In this section, we describe how we selected an image generation system and how we generated images from the participants' prompts.

System selection We experimented with different text-to-image generation systems, including CLIP Guided Diffusion (512x512, Secondary Model) 5 , CLIP Guided Diffusion (HQ 512x512 Uncond) 6 , DALLE-E mini 7 , Disco Diffusion 5.3 and 5.4 8 , Latent Diffusion 9 , and Majesty Diffusion 1.3 10 . In the end, we selected

Latent Diffusion for two main reasons. Latent Diffusion is the foundation for many of the community-driven adaptations and modifications. More importantly, the system is deterministic and leads to reproducible outcomes. Consecutive runs with the same seed value will generate the same images. This is a crucial requirement since we aim to compare images in between studies.

Image generation We generated images for the participants' prompts with Latent Diffusion using the following configuration settings: text2img-large model (1.4B parameters), seed value 1040790415, eta 1.0, ddim steps 100, and scale 5.0. Even though the system is capable of generating images at higher resolutions, we decided to generate images of 256 × 256 pixels to avoid the quirks that often occur when generating images in resolutions that the model was not trained on. The image generation job yielded 1875 images (125 participants × 3 prompts per participant × 5 images per prompt). After collecting the revised prompts from participants, we generated another set of 1875 images using the same seed value and configuration settings as before. Negative terms were used in this second set, if provided by the participant.

Some hand-selected images generated from the prompts are depicted in Figure 4. Many images were of photo-realistic quality, depicting landscapes, sunsets, beaches, and animals. Besides photographs, artistic styles included paintings, graphic designs, abstract artworks, as well as magazine and book covers. Some images contained text and many images contained watermarks.

## 5.1.3 Analysis

We analyzed the two sets of prompts and images written in studies 2 and 3 as follows.

Analysis of prompts To measure the amount of changes in the prompts, we calculated the number of tokens added and removed using parts-of-speech tagging as well as the Levenshtein distance (Levenshtein, 1965), a measure of lexical similarity denoting the minimum number of edits needed to change one string into another. The Levenshtein distance provides a simple measure for us to describe how much a prompt has changed in between the two studies. This is important to know, because it also reflects the participant's satisfaction with the generated output. Clearly, there will be satisficers and maximizers among participants (Schwartz et al., 2002). Some participants spend more time on prompt writing than others. We argue it is important to have a notion of the change in between prompts, measured by the Levenshtein distance. To understand the nature of the changes, the first author inductively developed a coding scheme (Hsieh & Shannon, 2005) with eight categories: adjectives/adverbs, subjects, prepositions, paraphrasing/synonyms, reordering, cardinal numbers, simplification, and presence of prompt modifiers. After discussing the codes among all authors and revising the codes, the first author coded all prompts and generated a co-occurrence matrix of changes made by participants. Note that we understand 'subjects' in the sense of subject terms (Oppenlaender, 2023) for image generation (e.g. 'a woman holding a phone' would have two subjects (woman and phone). Synonyms were analyzed at the level of individual words and parts of sentences.

Figure 4: Selected exemplars of a) successful and b) failed image generations from worker-provided prompts. The images in Figure 4a were selected to represent a variety of different styles and are not representative of the whole set of images. The images in Figure 4b depict some of the recurring issues in images generated from worker-provided prompts.

<!-- image -->

Analysis of the revised images We evaluated the images according to the following process, developed collaboratively by the authors. Initially, a spreadsheet was created with the two sets of prompts and their respective five images from Studies 2 and 3. Through a detailed discussion of 30 image-text pairs, the authors developed a set of evaluation criteria grounded in both the study's objectives and relevant literature. Similar criteria have been used in image evaluation studies, such as (Dai et al., 2023). This approach ensured a balance between the specificity of the study and established methodologies in image evaluation. The criteria were designed to encompass both objective and subjective aspects of the images, including binary categories for failed image generations, the extent of style and subject change, and improvements in consistency. Additionally, we included ratings for details, contrast, color, distortions, watermarks, and an overall subjective impression of quality. We acknowledge that while some elements of the evaluation were inherently subjective, they were rooted in discussions among the authors to ensure they were relevant and appropriate for the context of this study. We aimed to create a comprehensive evaluation framework that

not only aligns with existing standards but also caters to the unique aspects of AI-generated imagery.

Using the evaluation criteria, each author then individually rated 50 pairs of images along these criteria. After this initial round of coding, the authors discussed the results and decided to add four more criteria to the coding scheme. The final set of criteria included binary categories for failed generations, amount of style and subject change, and whether consistency improved, as well as ratings for details, contrast, color, distortions, watermarks, and overall subjective impression of quality. After a second round of coding, the authors cross-checked their evaluations and resolved differences through discussion.

## 5.2 Results

## 5.2.1 Participants

The sample consisted of 50 participants (40% of the participants who participated in Study 2). Participants included 25 men, 24 women, and 1 person who preferred not to disclose the gender identity, aged 20 to 71 years ( M = 42 . 76, SD = 14 . 63). Participants came from varied educational backgrounds, including some completed college courses (17 participants), Bachelor's degrees (22 participants), Master's degrees (4 participants), and doctorate degrees (2 participants). Seven out of ten participants had an educational

Figure 5: Background of the crowd workers participating in Study 3.

<!-- image -->

- § From 1 - Strongly Disagree to 5 - Strongly Agree
- ‡ 1 - Never, 2 - Rarely, 3 - Sometimes, 4 - Often, 5 - Very often
- † 1 - Not at all experienced, 2 - Slightly experienced, 3 - Moderately experienced, 4 - Very experienced, 5 - Extremely experienced

background in the arts. Some participants were interested in visiting museums and AI-generated imagery, but most did not practice art themselves and 80% had little or no experience with text-to-image generation.

Approximately 40% of participants were disappointed with the generated images, while 55% of participants' expectations were met. Around 60% of participants believed the images needed improvement, and a similar percentage of participants were confident that their revised prompts would improve the generated images.

## 5.2.2 Participants' revised prompts

The average Levenshtein distance between the participants' two prompts (not including negative terms) was 28.1 ( SD = 25 . 0). A computational analysis of the changes with parts-of-speech tagging shows that

participants added over twice as many tokens as they removed - 538 added tokens versus 243 removed tokens (see Figure 7a). Nouns were added most often (29.55% of added tokens), followed by adjectives (22.12%), prepositions (17.84%) and determiners (8.55%). The same types of tokens were also most often removed (28.81% of removed tokens were nouns, 16.87% prepositions, 13.17% adjectives, and 8.64% determiners). In 11 prompts (7.33%), the participant neither changed the prompt nor provided a negative term. Six of these instances consisted of participants pasting random snippets of text.

Table 3: Evaluation of changes in the two sets of images generated from prompts in Study 3.

|        | details contrast   | color      |            | watermarks   | consistency   | overall    |
|--------|--------------------|------------|------------|--------------|---------------|------------|
| worse  | 17 (11.3%)         | 17 (11.3%) | 12 (8.0%)  | 31 (20.7%)   | 23 (15.3%)    | 23 (15.3%) |
| same   | 81 (54.0%)         | 85 (56.7%) | 88 (58.7%) | 85 (56.7%)   | 95 (63.3%)    | 77 (51.3%) |
| better | 52 (34.7%)         | 48 (32.0%) | 50 (33.3%) | 34 (22.7%)   | 32 (21.3%)    | 50 (33.3%) |

Figure 6: Examples of changes (highlighted in bold) in adjectives and adverbs (a-c), subjects (d-f) and multiple changes at once (g-i) made by crowd workers to their own prompts in Study 4.

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

bright full moon just rising over the desert

bright full amber colored moon just rising over the desert

(a)

A comfortable warm blanket resting on an antique rocking chair.

A comfortable warm blanket resting on an antique rocking chair in front of a fireplace .

(d)

A fruit bowl with vibrant colored fruits in it and a contrasting background

A neutral colored bowl with a variety of several brightly colored and vibrant fruits in it, and a background that is darker to contrast with the fruit .

(g)

A famers market in Nebraska in the early fall.

An outdoor famers market in Nebraska in the early fall.

(b)

Are you able to produce any of rodans work.

Will you please correct Rodans art work for me .

(e)

A man in a blue busi-

ness

suit

sitting

on a

bench. He holds a brief-

case.

A man seated on a park bench. He is in a blue business suit. A briefcase is beside him on the bench.

(h)

A forest with scary trees all around

A forest with vibrant green trees all around.

(c)

miracle of creation

explosion of creativity

(f)

A wild cat sitting on a brightly-painted fence.

A tiger stands on top of a fence that has been painted with vivid primary colors.

(i)

Our coding showed that the main strategy used by participants was modifying (i.e., adding, removing, or switching) adjectives in their prompts (see Figure 7b). For example, a participant changed the prompt

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

(a) Histogram of changes in tokens

Figure 7: Participants added more tokens than they removed in Study 3 (Figure 7a). Figure 7b depicts the changes that often co-occurred with one another. For instance, a change (addition or removal) to an adjective often co-occurred with changes to other adjectives in the prompt.

<!-- image -->

'flowers in winter' to 'purple flowers in winter.' This was often combined with changes to the subject of the prompt (cf. Figure 6), such as changing 'sweeping arcs' to 'deep and broad, sweeping arcs in landscapes.' Some participants also adapted their prompts based on what they saw in the images, though this often resulted in only minor changes to the revised images. For instance, in the case of the above participant, the two images of mountainous landscapes were almost identical. Another common approach was changing prepositions in the prompts. Few participants attempted to simplify their prompts, and relatively few made changes to cardinal numbers. For instance, one participant changed 'draw a bunch of circles' to 'draw at least 15 circles,' and another participant wanted to see 'lots of puffy clouds' without specifying the exact number.

We found that only one participant (the same as in Section 4.2.3) demonstrated knowledge of prompt modifiers in all three of her prompts. An example written by this participants is 'rainbow tyrannosaurus rex, ✿✿✿✿✿✿✿✿✿ prehistoric ✿✿✿✿✿✿✿✿✿ landscape, ✿✿✿✿✿ studio ✿✿✿✿✿ ghibli, ✿✿✿✿✿✿✿ trending ✿✿✿ on ✿✿✿✿✿✿✿✿✿ artstation .' This participant used the underlined prompt modifiers which are commonly used in the AI art community. Only one other participant used a style modifier ( 'real photos of [...]' ) in one prompt. This shows a very small increase in the use of prompt modifiers among participants in between Study 2 and Study 3, even though participants were specifically instructed to improve their artworks.

## 5.2.3 Participants' revised images

We compared the two sets of images generated from each participant's prompts and found that over half of the revised sets showed no improvement in image quality (in terms of details, contrast, color, distortions, watermarks, and consistency). Selected changes in the prompts and the resulting images are depicted in Figure 6. About half of the sets remained the same, 15% were worse, and a third were better compared to

the previous set.

Some participants were able to make improvements to the generated images, mainly by adding more details. Since participants added more tokens than they removed, the prompts were longer and resulted in about a third of the images having more details. Some participants also improved the images' colors and contrast by adding adjectives to the prompts. For instance, one participant improved the amount of details by adding 'coral reef' to the end of the prompt 'scuba diver exploring unknown ocean.' This change resulted in less blur and more details in the coral reef. However, strong changes in the style of the images were rare, with about 70% of the revised sets being in the same or very similar style. Because participants did not use style modifiers, the revised images often resembled the initial images.

About 15% of the images were of low aesthetic quality, often consisting of text with no discernible subject (see Figure 4b). These images were rarely improved between the studies, and when they were, it was often due to chance. For instance, the subject was completely changed in about 10% of the images. This was often a result of participants trying to have a conversation with the AI and entering a completely different prompt as input (see Figure 6b and Figure 6e).

## 5.2.4 Participants' use of negative terms

Nineteen participants used negative terms, with eight using them in all three prompts. In total, we collected 39 negative terms. Many negative terms ( n = 19) aimed at removing or modifying the subject in various ways, such as removing 'rocks' from a beach, trying to correct a 'weird face,' avoiding a 'Nude, Naked, White, Man,' removing the color 'Green.' in the image of a red star, or attempting to change the subject entirely ( 'ballroom' ). Participants tried to change the style of the images in eight cases, using terms such as 'Black, White, Colorless, Monochromatic,' 'opaque, solid,' and 'unfocused.' Four out of the 50 participants tried to remove text in the images, using negative terms such as 'letters,' 'captions,' and 'text.' Only one participant attempted to remove watermarks, using the negative term 'remove watermark.' As can be seen in this prompt, some participants did not understand the concept of negative terms, even though we explained it to them. A few examples of failed and successful attempts are depicted in Figure 8. Some of the image generations failed, because the participant did not use the negative term correctly. For instance, the prompt on the bottom right of Figure 8 contains a monarch butterfly both in the prompt and negative term. The resulting image is sub-par compared to the image generated from the participant's original prompt.

## 6 Discussion

In three studies, we explored the skill of prompt engineering with 227 participants recruited from a crowdsourcing platform. Our first study shed light on whether laypeople have an understanding of what makes a 'good' prompt. Our findings indicate that participants can assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art. In the subsequent

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

revised prompt An explosively bright, dying star. [Green]

<!-- image -->

medium sized metallic sphere slightly above and left of the center of the image [remove watermark]

An autumn day in a colorful forest [Dark Circle]revised prompt Hummingbird over red flower with a out of focus background of green shrubs [Hummingbird hovering over red flower with short focal point.]

<!-- image -->

Rainbow colored unicorn with huge mane jumpin over the full moon [Shimmering hairy unicorn jumping over the full moon]

Butterfly Monarch on a flower [Monarch butterfly alit on a wildflower]

before

<!-- image -->

<!-- image -->

<!-- image -->

Figure 8: Examples of successful (left) and failed (right) attempts of participants using negative terms (in square brackets).

<!-- image -->

two studies on writing and improving prompts, we found that participants wrote creative prompts in rich descriptive language which may result in beautiful digital artworks. However, only a negligible amount of participants applied their knowledge of art in practice and failed to use terms commonly applied in communities of text-to-image art, such as Midjourney and Stable Diffusion. With the exception of one participant, these specific keywords were not (yet) part of the vocabulary of crowdsourced participants on MTurk at the time of our study. While the prompts written by participants were very descriptive and, in some cases, resulted in beautiful and interesting images, participants left the style of their image to chance. The prompts were missing modifiers that would more tightly control the style and quality of the image generation. This applies to both the initial study on writing prompts and the study on revising and improving prompts. In the latter, only a minority of the participants were able to improve their revised images, while most of the images remained about the same quality. An overwhelming majority of participants left the style of the generated images to chance, even though they were specifically instructed to create 'artworks.'

Reflecting on these findings, it becomes evident that while crowd engagement with AI-driven art generation shows promise, there is a noticeable gap in the effective use of prompt engineering. This leads us to consider the broader implications and questions within the AI and Human-AI Interaction research fields. Particularly, it raises the question of whether prompt engineering is an intuitive skill that can be easily acquired or if it requires more specialized training and understanding.

## 6.1 Prompt Engineering as a Non-Intuitive Skill

Our work adds to the discussion of broader research questions in the AI and Human-AI Interaction research communities: Can anyone become an artist with prompt engineering? Is prompt engineering a skill that is innate to us humans or is it a skill that needs to be acquired through practice and learning? If prompt

<!-- image -->

engineering is an intuitive skill, how intuitive is it? Or in other words, how steep is the learning curve to prompt engineering? These research questions have implications for the future of work and humancomputer co-creativity (Kantosalo et al., 2020). If prompt engineering is an intuitive human skill that we humans can apply effortlessly, then we can look forward to a bright future where anyone can work in creative professions without having to develop special creative skills. But if prompt engineering is a non-intuitive skill, its application could become limited to highly trained and skilled class of creative professionals who have mastered to speak the language of the generative model through extensive training. The latter case could clearly negatively impact creative production and stifle innovation.

Prompting is a language-based practice and the use of language is intuitive to us humans. Therefore, one could assume that prompting is an intuitive skill. It is easy to get started with writing prompts and prompting has a large potential in different fields and for many application domains. However, our study found that effective prompt writing requires knowledge of keywords and key phrases. These prompt modifiers are an essential part of the skill of prompt engineering for AI generated art. Typically, these keywords and key phrases are acquired through iterative experimentation and by learning from prompts shared in dedicated resources, on social media, or in online communities (Oppenlaender, 2022). Our studies empirically confirm that style modifiers are unknown to participants recruited on Amazon Mechanical Turk. Prompt modifiers that are being used profusely in the AI art community have not found their way into the collective vocabulary of crowdsourced participants on MTurk. Participants in our study struggled to write and improve their prompts for the specific task of creating digital artworks. This points towards prompt engineering being a non-intuitive skill or perhaps even a specialist skill.

However, we acknowledge this is a simplistic view. One repeated interaction with the generative model does not lead to learning effects and a skill requires learning to be mastered. Perhaps a different question to ask is whether prompt engineering is a skill at all. The initial outputs of text-to-image generation systems have a high randomness and, due to the ineffectiveness of discrete language for describing images in detail, it is very difficult to control the initial image. Therefore, text-to-image generation often requires several iterations to get to an acceptable level. However, in practice, this first image is just the starting point. There is much more to prompt engineering than just writing textual prompts. In particular, the first image is generally only the start in a longer creative process that can include, for instance, ControlNet, image editors, image-to-image generation, inpainting and outpainting, facial detailing, and upscaling. Upscaling in itself is often completed with generative models, such as Real-ESRGAN (Wang, Xie, Dong, & Shan, 2021), thereby introducing artifacts that may not be wanted, requiring further editing. Prompt engineering is not just tied to the use of a platform, but to a whole ecosystem of creative tools (e.g., image editors and generative models). Learning this variety of tools requires specific training. We speculate on four possible futures for the skill of prompt engineering in the following section.

## 6.2 On the Future of Creative Production with Prompt Engineering

Text-to-image generation opens new opportunities for creative production of digital images and artworks. Whether prompt engineering will become an expert skill or even a novel profession is still open. In this section, we speculate on four possible futures of prompt engineering.

## 6.2.1 Prompt engineering as an expert skill

In the future, prompt engineering could become an expert skill that requires deep subject-matter expertise (e.g., knowledge of subject-specific keywords, prompt modifiers, and their combinations, but also of the idiosyncrasies of the training data and system configuration parameters) to effectively control the output of generative systems. This is similar to the move in the field of machine learning towards 'foundation models' (Bommasani et al., 2021). Foundation models are very large and costly to train, operate, and maintain. As a result, the creation of these models as well as research on these models is limited to a small number of well-financed research institutes that employ highly-skilled professionals working in well-funded research institutes and organizations. If prompt engineering becomes a highly skilled profession, it may become exclusive to a narrow group of privileged individuals who have undergone extensive training.

In our study (conducted in mid-2022), only one participant demonstrated knowledge of prompt modifiers. Controlling text-to-image generation is still a difficult task, and practitioners spend many hours obsessing over very small details. In fact, as described in the previous section, the workflow of text-to-image generation typically involves more steps than just prompt writing - this can be considered only the first steps in a complex task work flow that involves image generation models as well as editors and specialized tools. From this perspective, text-to-image generation remains an expert skill that, while now accessible to a large part of the population, remains difficult to master. This perspective is supported by the finding that most images generated in this study failed to achieve the given goal of creating an 'artwork' (without defining what an artwork is and, granted, acknowledging that photographs are also art). However, prompt modifiers that would nudge the text-to-image generation system to produce artworks, whether photography or oil painting, were notably absent in the prompts written by the participants in our study.

## 6.2.2 Prompt engineering as an everyday skill

In the future, prompt engineering could become a common practice. In this scenario, people would adapt their creative practices and language to facilitate effective interaction with AI because it is a skill that is needed in everyday life. People have a need for visual content, and AI-generated content could satisfy this need, from internet memes to the design of greeting cards, logos, and artworks. Prompt engineering could also be used for self-actualization, creativity, and therapy to improve mental health and well-being (Burleson, 2005). In this scenario, people would expand their existing vocabulary to include terms used in prompt engineering in order to produce meaningful outcomes with generative systems. Learning prompt engineering would be similar to learning a new language. This skill would be acquired at an early age, just

like internet literacy is today acquired effortlessly by the generation born after the internet found widespread use. The skill could also become part of media literacy education in schools to elevate the common skill-level to a professional level.

There is some support in our studies for this speculative future. Most participants were able to write creative and detailed prompt. Digital literacy in using text-to-image generation systems can therefore be assumed to be present to some degree. With text-to-image technology improving, the skill of prompt engineering may become easier and not require as many special keywords and modifiers in the prompts. On the other hand, if this skill was to be used daily, participants would likely want to improve their skill to make use of prompt engineering more efficiently. Our study itself is an example of how rapidly the world changed after the wide-spread release of generative AI. It is likely that our study would have a different outcome today, simply because more people, today, know how to write basic prompts and perform basic prompt engineering. This would support the perspective of prompt engineering becoming a basic everyday skill.

## 6.2.3 Prompt engineering as an obsolete skill

In the future, prompt engineering could become irrelevant. How users interact with AI models is closely coupled with the advances of AI technology and what the AI models are capable of. Prompt engineering can be seen as the smell of a half-baked product that does not solve its users' needs. As generative systems improve their ability to understand the intent of users, prompt engineering could be a short-lived trend. The problem of aligning AI with human intent is known as AI alignment in the scholarly literature (Gabriel, 2020). State-of-the-art systems, such as ChatGPT (OpenAI, 2022) and DALL-E 3 (Betker et al., 2023), demonstrate impressive performance in understanding textual input prompts and aligning with user intent. With these systems, users of all skill levels can generate content from textual prompts. As generative systems become better at understanding user intent, prompt engineering could become obsolete, similar to how we no longer use block printing. Prompt engineering could become unnecessary - an archaic skill that does not require expert training and that only few people exercise for nostalgic reasons.

Support for this speculative future comes primarily from the creators of generative tools themselves. One example is Midjourney which introduced a noticeable jump in quality between version 5 and 6, with the latter version requiring fewer keyword modifiers in prompts (Midjourney, 2023). The CEO of Midjourney, David Holz, has recommended not to put too much effort into acquiring the skill of prompt engineering because generative systems change too quickly (Acar, 2023; Holz, 2023). Another example of the skill potentially becoming obsolete are language models that generate prompts. This approach is used in some text-to-image generation systems to rewrite the user input and improve the prompt quality. One instance of such LMbased prompt rewriting is used on the image generator ideogram.ai. Such prompt rewriting could, if language models were well aligned with user intent, make the skill of prompt engineering obsolete.

## 6.2.4 Prompt engineering as personal signature or curation skill

In the future, our human senses could become better at distinguishing hand-crafted art from AI-generated digital art. AI artist Mario Klingemann speculated that with the influx of AI-generated images, this skill would help us notice subtle nuances, details, and imperfections in AI-generated art, which could become more important in determining the aesthetic quality of an art piece. 11 In this scenario, anyone could write prompts for generative systems with good results, but only a few would become masters of prompt engineering. The practice of prompt engineering would remain a necessary skill for applying finishing touches and optimizing generative results, as well as imbuing an artwork with a personal style to distinguish it from bland 'off-theshelf' generations. Alternatively or in parallel, prompt engineering could evolve into a curation skill - a personal practice in which everyone has their own curated sets of textual and visual inputs used to finetune generative models for different purposes. In this scenario, the machine would personalize and adapt to humans, rather than the other way around. As a result, the generative machine would potentially develop a perfect understanding of user intent. Current approaches that cater towards this future are Low-Rank Adaptation (LoRA) (Hu et al., 2022), Dreambooth (Ruiz et al., 2023) and textual inversion (Gal et al., 2022).

There is some support for this possible future in our study. Participants were able to tell bad quality prompts from good quality prompts. This could enable participants to curate prompts. Furthermore, the prompts from some of our study participants had a clear personal signature. The idiosyncratic way of writing prompts was easily recognizable in some participants. Future generative models could pick up on these subtleties and adapt to the idiosyncrasies of their user-written prompts. This would, clearly, enable these models to outperform models that cannot adapt to the subtleties of user intent.

## 6.2.5 Review and outlook

These four speculative futures were initially formulated in 2022. Two years on, there has still not been a definitive answer to the question which potential future is more likely and in which direction we are heading. School curricula have still not adapted prompt engineering. Prompt engineering is a perishable skill with a short shelf life. Every time a model is updated, practitioners have to adopt new tricks and the skill of prompt engineering has to be learned anew. Even the CEOs of generative AI companies, such as Midjourney, recommend not putting too much effort into learning the skill of prompt engineering, because it is changing too rapidly (Holz, 2023; Midjourney, 2023). On the other hand, what is enduring is the continued public interest in prompt engineering and its usefulness for solving real-world problems, such as question-answering over text documents (Lewis et al., 2020; Oppenlaender & Hamalainen, 2023). LM-based agents have become popular in industry due to their usefulness in solving difficult problems, surpassing even web development frameworks in popularity (Oppenlaender, 2024; Wu et al., 2024). From this perspective, prompt engineering - and, more generally, AI engineering - is considered a valuable skill to have and teach in school curricula

(Microsoft, 2024). There are now job positions made available with title 'Prompt Engineer,' not only at technology companies, but also in the public sector, such as the position of a 'Senior Prompt Engineer' role at the AI Safety Institute of the UK's Department for Science, Innovation & Technology (Department for Science, Innovation & Technology, 2024). However, digging deeper into this particular job role, it is clear that the role of Prompt Engineer is more encompassing than just writing prompts - it requires technology skills, such as Python programming, and knowledge of deep learning and evaluation frameworks. Advanced prompting techniques are just one aspect in this job profile.

The rapidly evolving landscape of generative AI is the crux of the skill of prompt engineering. While the skill is useful for hobbyists and practitioners in industry, it is changing too rapidly to make it a sensible addition to school curricula. As a consequence, we can expect the gap between academia and practice to become even wider in the coming years. With this context in mind, we now turn our attention to future work.

## 6.3 Future Work

We conducted an experiment asking crowdsourced participants to write, improve, and assess prompts. The study was conducted in May-July 2022, at a time when text-to-image generation was still unknown to most people. Today, many people have tried text-to-image generation at least once, and many more have heard of the scandals and lawsuits surrounding generative AI. Knowledge of these scandals may shape the perception and use of generative AI (Zhu et al., 2024). Our study was conducted in mid-2022, and, today, it would be difficult to recruit a participant sample as naive to text-to-image generation as our sample. This makes our study important and valuable. This is precisely the value of our work: we evaluated prompt engineering as a skill with naive participants, at a time when text-to-image generation was not as popular as it is today. Midjourney was just released and still unknown to many people. Stable Diffusion was released months later, in August 2022. And ChatGPT was made available only later the same year. Our participant sample consisted of laypeople who were still naive to 'prompt engineering' for text-to-image generation. Such a participant sample is hard to find today - most people have heard and tried image generators or language models at least once, and ChatGPT is available for free. The interaction with image generators is typically interactive: a user enters a prompt, observes the results, and types another prompt in response to the observed output. Therefore, text-to-image generation has learning built in. It is, today, very difficult to disentangle this aspect of interactive learning from any investigation concerned with understanding the skill underlying prompt engineering. In our study, participants had no experience in text-to-image generation (except for one participant) and underwent one repeated interaction with the system. This allowed us to investigate the skill of prompt engineering in our experiment. Future work should build on this, for the reasons outlined in Section 6.2.5, even though recruiting naive participants is challenging. Is is pertinent to investigate whether prompt engineering is to be included in school and high education curricula. To make an informed decision, one would need to know more about how quickly the skill of prompt engineering is learned, what it exactly entails (besides prompt writing), the steepness of the learning curve, and how long

it would take to master this skill. All these factors are needed to make an informed decision of whether prompt engineering is a skill to include in curricula.

We investigated textual prompt engineering in our paper. But there are more facets to text-to-image generation in practice, as discussed in Section 6.1 and outlined in Oppenlaender (2022), such as visual prompting and configuration parameters in text-to-image generation systems. Future research should consider the complexity and diversity of skills and should always consider all facets involved in the creative process, not just textual prompts. Future work could envision how the machine can adapt to humans and possibly provide guidelines for AI researchers, so they can develop AI models that understand user intent and foster a more 'intimate' relationship with users (Weiser, 1993). Language - whether written or spoken - is the perfect vehicle for this intimate relationship with users.

## 6.4 Limitations

We acknowledge a number of risks to the validity of our exploratory studies. Aesthetic quality assessment, as explored in Study 1, inherently carries a high degree of subjectivity (Joshi et al., 2011). Various factors influence such ratings, including personal values, background, image content and its interestingness, contrast, proportion, number of elements, novelty, and appropriateness (Joshi et al., 2011; Khalighy, Green, Scheepers, & Whittet, 2014; Kong et al., 2016). We acknowledge that the task given to participants in Study 1 was challenging, particularly in terms of visualizing and evaluating the potential outcome of a written prompt. While our findings indicate that participants could discern between low and high-quality prompts, we must also recognize a key limitation: the absence of direct measurements for the actual quality of the generated artworks themselves. This gap points to the difficulty in quantitatively assessing artistic creations, a challenge further compounded by the subjective nature of aesthetic evaluation. Future research could benefit from developing more concrete and objective criteria or methods to assess the quality of prompt-artwork pairs, providing a more comprehensive understanding of the relationship between prompt quality and the resulting art.

We further acknowledge limitations in our choice of text-to-image generation system. Our main motivation for selecting Latent Diffusion was, at the time, that it was a state-of-the-art image generation system allowing the deterministic and reproducible generation of images. We tested a dozen of other text-to-image generation systems, notably CLIP-Guided Diffusion, GLID-3-XL, DALL-E mini (Craiyon), Latent Majesty Diffusion, DISCO Diffusion, and VQGAN-CLIP (Crowson et al., 2022), with mixed results. Some systems had non-deterministic (random) components leading to image generations not being reproducible. Other systems were not advanced enough, at the time, to produce recognizable results. Note, however, that while Latent Diffusion is a powerful image generation system, it may respond differently to style keywords than CLIP-guided systems. Our choice of latent diffusion was a compromise between reproducibility and performance. In any case, we argue the choice of image generation system does not matter much for our study. Our study setup did not provide interactive feedback to participants, and the system itself was not our subject

of study. Instead, we were interested in the intrinsic skill of participants. However, only one participant used specific keywords (prompt modifiers) in our study. The second round of images was also never shown to participants. Therefore, we can safely assert that the choice of system had no effect on how participants adapted to the generative system and how they wrote and revised their prompts.

Regarding the observation about participants making greater improvements in prompts but not in modifiers in Study 3, we realize that our initial instructions did not explicitly guide participants to modify the stylistic elements or modifiers of the prompts. This oversight might have led to less emphasis on altering these aspects, thus skewing the results in favor of more noticeable changes in the prompts' substantive content. However, the overall tasks given to the participant was to improve the artwork, and the instructions were thus implicitly part of the given task. As an alternative approach, a more directed instruction could have provided insights into how novice users interact with and perceive the importance of prompt modifiers in text-to-image generation. This realization could be a valuable consideration for the design of future studies.

Last, we acknowledge that to explore the dynamics of prompting skill learning, a long-term field study would need to be conducted. However, our one time modification experiment provides a first indication indicating that the skill of prompting is not an innate skill that users can apply without learning about it first. Recent work by Don-Yehiya, Choshen, and Abend (2023) provides intriguing insights on the dynamics of prompt learning on Midjourney that future work could build on.

## 7 Conclusion

The past few years have seen the rise of generative models. It is too early to tell whether this development will give birth to new professions, such as 'prompt engineer.' However, generative AI will deeply affect and reconfigure the fabric of our society. This opens exciting opportunities for research in the field of HCI.

This article investigated prompt engineering, as a new type of skill, in the context of AI art. In three studies, we investigated whether laypeople naive to text-to-image generation could recognize the quality of prompts and their resulting images, and whether participants could write and improve prompts, without repeated feedback from the image generation system. We found participants recruited from a crowdsourcing platform were creative and able to write prompts for text-to-image generation systems in rich descriptive language, but lacked the special vocabulary found in AI art communities. The use of prompt modifiers was not intuitive to participants, pointing towards prompt engineering being a non-intuitive skill. We discussed the importance of our study's findings, and speculated on four possible futures for prompt engineering. We hope that whatever the landscape of creative production will turn out to be in the future, it will be an inclusive creative economy in which everyone can participate in meaningful ways.

## References

Acar, O. A. (2023). AI prompt engineering isn't the future. Harvard Business Review . Retrieved from https://hbr.org/2023/06/ai-prompt-engineering-isnt-the-future

Allen, C. (2022). Zippy's disco diffusion cheatsheet v0.3. (https://docs.google.com/document/d/1l8s7uS2d GqjztYSjPpzlmXLjl5PM3IGkRWI3IiCuK7g/edit)

Arai, S., & Kawabata, H. (2016). Appreciation contexts modulate aesthetic evaluation and perceived duration of pictures. Art & Perception , 4 (3), 225 - 239. doi: 10.1163/22134913-00002052

Bach, S., Sanh, V., Yong, Z. X., Webson, A., Raffel, C., Nayak, N. V., . . . Rush, A. (2022, 5). PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th annual meeting of the association for computational linguistics: System demonstrations (pp. 93-104). Dublin, Ireland: Association for Computational Linguistics. Retrieved from https:// aclanthology.org/2022.acl-demo.9 doi: 10.18653/v1/2022.acl-demo.9

Bateman, S., Teevan, J., & White, R. W. (2012). The search dashboard: How reflection and comparison impact search behavior. In Proceedings of the sigchi conference on human factors in computing systems (p. 1785-1794). New York, NY, USA: Association for Computing Machinery. doi: 10.1145/2207676 .2208311

Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., . . . Ramesh, A. (2023). Improving image generation with better captions. Retrieved from https://cdn.openai.com/papers/dall-e-3.pdf Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., . . . Liang, P. (2021). On the opportunities and risks of foundation models. Retrieved from https://crfm.stanford.edu/assets/

## report.pdf

Burleson, W. (2005). Developing creativity, motivation, and self-actualization with learning systems. International Journal of Human-Computer Studies , 63 (4), 436-451. doi: 10.1016/j.ijhcs.2005.04.007

Covington, M. A., & McFall, J. D. (2010). Cutting the gordian knot: The moving-average type-token ratio (mattr). Journal of Quantitative Linguistics , 17 (2), 94-100. doi: 10.1080/09296171003643098

Crowson, K., Biderman, S., Kornis, D., Stander, D., Hallahan, E., Castricato, L., & Raff, E. (2022). VQGANCLIP: Open domain image generation and editing with natural language guidance. In S. Avidan, G. Brostow, M. Ciss'e, G. M. Farinella, & T. Hassner (Eds.), Computer vision - eccv 2022 (pp. 88105). Cham: Springer Nature Switzerland.

Dai, X., Hou, J., Ma, C.-Y., Tsai, S., Wang, J., Wang, R., . . . Parikh, D. (2023). Emu: Enhancing image generation models using photogenic needles in a haystack.

Dang, H., Goller, S., Lehmann, F., & Buschek, D. (2023). Choice over control: How users write with large language models using diegetic and non-diegetic prompting. In Proceedings of the 2023 chi conference on human factors in computing systems. New York, NY, USA: Association for Computing Machinery. doi: 10.1145/3544548.3580969

Dang, H., Mecke, L., Lehmann, F., Goller, S., & Buschek, D. (2022). How to prompt? Opportunities and

challenges of zero- and few-shot learning for human-AI interaction in creative applications of generative models. In Generative AI and HCI workshop at CHI 2022. doi: 10.48550/ARXIV.2209.01390

Deckers, N., Frobe, M., Kiesel, J., Pandolfo, G., Schroder, C., Stein, B., & Potthast, M. (2023). The infinite index: Information retrieval on generative text-to-image models. In Proceedings of the 2023 conference on human information interaction and retrieval (p. 172-186). New York, NY, USA: Association for Computing Machinery. doi: 10.1145/3576840.3578327

Department for Science, Innovation & Technology. (2024). Senior prompt engineer - autonomous systems (ai safety institute). Retrieved from https://archive.today/bhV6I

Dhariwal, P., & Nichol, A. Q. (2021). Diffusion models beat GANs on image synthesis. In A. Beygelzimer,

- Y. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in neural information processing systems. Retrieved from https://openreview.net/forum?id=AAWuCvzaVt

Don-Yehiya, S., Choshen, L., & Abend, O. (2023). Human learning by model feedback: The dynamics of iterative prompting with midjourney.

Gabha, H. (2022). Disco diffusion 70+ artist studies. Retrieved from https://weirdwonderfulai.art/ resources/disco-diffusion-70-plus-artist-studies/

Gabriel, I. (2020). Artificial intelligence, values, and alignment. Minds and Machines , 30 (3), 411-437. doi: 10.1007/s11023-020-09539-2

Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., & Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv. doi: 10.48550/ARXIV.2208.01618

Gerger, G., Leder, H., & Kremer, A. (2014). Context effects on emotional and aesthetic evaluations of artworks and IAPS pictures. Acta Psychologica , 151 , 174-183. doi: 10.1016/j.actpsy.2014.06.008

Grassini, S., & Koivisto, M. (2024). Artificial creativity? Evaluating AI against human performance in creative interpretation of visual stimuli. International Journal of Human-Computer Interaction , 0 (0), 1-12. doi: 10.1080/10447318.2024.2345430

Haddington, P., Hirvonen, N., Hosio, S., Kinnula, M., Malmberg, J., Seyfi, S., . . . Zabolotna, K. (2021). GenZ white paper: Strengthening human competences in the emerging digital era (Tech. Rep.). University of Oulu. Retrieved from http://jultika.oulu.fi/files/isbn9789526231471.pdf

Hayward, J. (2022, 9 17). The growing art movement of 'promptism'. Retrieved from https://medium.com/ counterarts/the-growing-art-movement-of-promptism-9ec956d82a61

Hoare, M., Benford, S., Jones, R., & Milic-Frayling, N. (2014). Coming in from the margins: Amateur musicians in the online age. In Proceedings of the sigchi conference on human factors in computing systems (p. 1295-1304). New York, NY: Association for Computing Machinery. doi: 10.1145/2556288 .2557298

Holz, D. (2023). Midjourney office hours.

Personal communication via Discord.

H. Onan Demirel, X. L., Molly H. Goldstein, & Sha, Z. (2024). Human-centered generative design framework:

An early design framework to support concept creation and evaluation. International Journal of Human-Computer Interaction , 40 (4), 933-944. doi: 10.1080/10447318.2023.2171489

Hope, T., Tamari, R., Hershcovich, D., Kang, H. B., Chan, J., Kittur, A., & Shahaf, D. (2022). Scaling creative inspiration with fine-grained functional aspects of ideas. In Chi conference on human factors in computing systems. New York, NY: Association for Computing Machinery. doi: 10.1145/3491102 .3517434

Hou, Y., Dong, H., Wang, X., Li, B., & Che, W. (2022, 10). MetaPrompting: Learning to learn better prompts. In Proceedings of the 29th international conference on computational linguistics (pp. 32513262). International Committee on Computational Linguistics. Retrieved from https://aclanthology .org/2022.coling-1.287

Hsieh, H.-F., & Shannon, S. E. (2005). Three approaches to qualitative content analysis. Qualitative health research , 15 (9), 1277-1288.

Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., . . . Chen, W. (2022). LoRA: Low-rank adaptation of large language models. In International conference on learning representations. Retrieved from https://openreview.net/forum?id=nZeVKeeFYf9

Huang, J., & Efthimiadis, E. N. (2009). Analyzing and evaluating query reformulation strategies in web search logs. In Proceedings of the 18th acm conference on information and knowledge management (p. 77-86). New York, NY, USA: Association for Computing Machinery. doi: 10.1145/ 1645953.1645966

Jeong Soo Kim, M. K., & Baek, T. H. (2024). Enhancing user experience with a generative AI chatbot. International Journal of Human-Computer Interaction , 0 (0), 1-13. doi: 10.1080/10447318.2024.2311971 Jiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M., & Cai, C. J. (2022). Promptmaker: Prompt-based prototyping with large language models. In Extended abstracts of the 2022 chi conference on human factors in computing systems. New York, NY, USA: Association for Computing Machinery.

- doi: 10.1145/3491101.3503564

Johnson, W. (1944). Studies in language behavior 1: A program of research. Psychological Monographs , 56 , 1-15.

- Joshi, D., Datta, R., Fedorovskaya, E., Luong, Q.-T., Wang, J. Z., Li, J., & Luo, J. (2011). Aesthetics and emotions in images. IEEE Signal Processing Magazine , 28 (5), 94-115. doi: 10.1109/MSP.2011.941851

Kantosalo, A., Thattai Ravikumar, P., Grace, K., & Takala, T. (2020, 9 7). Modalities, styles and strategies:

An interaction framework for human-computer co-creativity. In A. Cardoso, P. Machado, T. Veale, &

- J. Cunha (Eds.), Proceedings of the eleventh international conference on computational creativity (pp. 57-64). Association for Computational Creativity.

Khalighy, S., Green, G., Scheepers, C., & Whittet, C. (2014). Measuring aesthetic in design. In Proceedings of the DESIGN 2014 13th international design conference (p. 2083-2094). The Design Society.

Kim, S., Eun, J., Oh, C., & Lee, J. (2024). 'journey of finding the best query': Understanding the user

experience of AI image generation system. International Journal of Human-Computer Interaction , 1-19.

Kong, S., Shen, X., Lin, Z., Mech, R., & Fowlkes, C. (2016). Photo aesthetics ranking network with attributes and content adaptation. In B. Leibe, J. Matas, N. Sebe, & M. Welling (Eds.), Computer vision - eccv 2016 (pp. 662-679). Cham: Springer International Publishing.

Kugler, L. (2021, 8). Non-fungible tokens and the future of art. Commun. ACM , 64 (9), 19-20. doi: 10.1145/3474355

Kyle, K. (2018). lexical-diversity python package. Retrieved from https://github.com/kristopherkyle/ lexical diversity

Lee, S.-y., Law, M., & Hoffman, G. (2024). When and how to use AI in the design process? implications for human-AI design collaboration. International Journal of Human-Computer Interaction , 1-16. doi: 10.1080/10447318.2024.2353451

Levenshtein, V. I. (1965). Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics. Doklady , 10 , 707-710.

Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., . . . Kiela, D. (2020). Retrievalaugmented generation for knowledge-intensive NLP tasks. In Proceedings of the 34th international conference on neural information processing systems. Red Hook, NY, USA: Curran Associates Inc.

Liu, V., & Chilton, L. B. (2022). Design guidelines for prompt engineering text-to-image generative models. In Chi conference on human factors in computing systems. New York, NY: Association for Computing Machinery. doi: 10.1145/3491102.3501825

McCarthy, P. M., & Jarvis, S. (2010). Mtld, vocd-d, and hd-d: A validation study of sophisticated approaches to lexical diversity assessment. Behavior Research Methods , 42 (2), 381-392. doi: 10.3758/brm.42.2 .381

McDonald, N., Schoenebeck, S., & Forte, A. (2019, 11). Reliability and inter-rater reliability in qualitative research: Norms and guidelines for CSCW and HCI practice. Proc. ACM Hum.-Comput. Interact. , 3 (CSCW). doi: 10.1145/3359174

Merriam-Webster. (2024). Skill. Retrieved from https://www.merriam-webster.com/dictionary/skill Microsoft. (2024, 5 8). 2024 work trend index annual report (Tech. Rep.). Author. Retrieved from https://www.microsoft.com/en-us/worklab/work-trend-index/ai-at-work-is

## -here-now-comes-the-hard-part

Midjourney. (2022). Midjourney.com. Retrieved from https://www.midjourney.com

Midjourney. (2023, 12 21). Announcement. Retrieved from https://discord.com/channels/ 662267976984297473/952771221915840552/1187272332268556298

OpenAI. (2022). ChatGPT: Optimizing language models for dialogue. Retrieved from https://openai.com/ blog/chatgpt/

Oppenlaender, J. (2022). The creativity of text-to-image generation. In 25th international academic mindtrek

conference (p. 192-202). New York, NY, USA: Association for Computing Machinery. doi: 10.1145/ 3569219.3569352

Oppenlaender, J. (2023). A taxonomy of prompt modifiers for text-to-image generation. Behaviour & Information Technology , 1-14. doi: 10.1080/0144929X.2023.2286532

Oppenlaender, J. (2024). The cultivated practices of text-to-image generation.

doi: 10.48550/arXiv.2306.11393

Oppenlaender, J., Abbas, T., & Gadiraju, U. (2024, 4). The state of pilot study reporting in crowdsourcing: A reflection on best practices and guidelines. Proc. ACM Hum.-Comput. Interact. , 8 (CSCW1). doi: 10.1145/3641023

Oppenlaender, J., & Hamalainen, J. (2023). Mapping the challenges of HCI: An application and evaluation of ChatGPT and GPT-4 for mining insights at scale. doi: 10.48550/arXiv.2306.05036

Oppenlaender, J., Silvennoinen, J., Paananen, V., & Visuri, A. (2023). Perceptions and realities of text-toimage generation. In Proceedings of the 26th international academic mindtrek conference (p. 279-288). New York, NY, USA: Association for Computing Machinery. doi: 10.1145/3616961.3616978

Oppenlaender, J., Visuri, A., Paananen, V., Linder, R., & Silvennoinen, J. (2023). Text-to-image generation: Perceptions and realities. In Workshop on generative AI and HCI at CHI '23. doi: 10.48550/arXiv .2303.13530

Pelowski, M., Gerger, G., Chetouani, Y., Markey, P. S., & Leder, H. (2017). But is it really art? The classification of images as 'art'/'not art' and correlation with appraisal and viewer interpersonal differences. Frontiers in Psychology , 8 . doi: 10.3389/fpsyg.2017.01729

Pinson, M. H., Janowski, L., Pepion, R., Huynh-Thu, Q., Schmidmer, C., Corriveau, P., . . . Ingram, W. (2012). The influence of subjects and environment on audiovisual subjective tests: An international study. IEEE Journal of Selected Topics in Signal Processing , 6 (6), 640-651. doi: 10.1109/JSTSP.2012 .2215306

Qiao, H., Liu, V., & Chilton, L. (2022). Initial images: Using image prompts to improve subject representation in multimodal AI generated art. In Creativity and cognition (p. 15-28). New York, NY: Association for Computing Machinery. doi: 10.1145/3527927.3532792

Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., . . . Sutskever, I. (2021, 7). Learning transferable visual models from natural language supervision. In M. Meila & T. Zhang (Eds.), Proceedings of the 38th international conference on machine learning (Vol. 139, pp. 8748-8763). PMLR.

Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-conditional image generation with CLIP latents. arXiv. doi: 10.48550/ARXIV.2204.06125

Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., . . . Sutskever, I. (2021, 7). Zero-shot textto-image generation. In M. Meila & T. Zhang (Eds.), Proceedings of the 38th international conference on machine learning (Vol. 139, pp. 8821-8831). PMLR. Retrieved from https://proceedings.mlr

## .press/v139/ramesh21a.html

Reynolds, L., & McDonell, K. (2021). Prompt programming for large language models: Beyond the few-shot paradigm. In Extended abstracts of the 2021 chi conference on human factors in computing systems. New York, NY, USA: Association for Computing Machinery. doi: 10.1145/3411763.3451760

Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In 2022 ieee/cvf conference on computer vision and pattern recognition (cvpr) (p. 10674-10685). doi: 10.1109/CVPR52688.2022.01042

Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023, 6). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (p. 22500-22510).

Schwartz, B., Ward, A., Monterosso, J., Lyubomirsky, S., White, K., & Lehman, D. R. (2002). Maximizing versus satisficing: Happiness is a matter of choice. Journal of Personality and Social Psychology , 83 (5), 1178-1197. doi: 10.1037/0022-3514.83.5.1178

Siahaan, E., Redi, J. A., & Hanjalic, A. (2014). Beauty is in the scale of the beholder: Comparison of methodologies for the subjective assessment of image aesthetic appeal. In 2014 sixth international workshop on quality of multimedia experience (qomex) (p. 245-250). doi: 10.1109/QoMEX.2014.6982326

Smith, E. (2022). A traveler's guide to the latent space. Retrieved from https://sweet-hall-e72.notion .site/A-Traveler-s-Guide-to-the-Latent-Space-85efba7e5e6a40e5bd3cae980f30235f

- Surowiecki, J. (2005). The wisdom of crowds . Anchor.

Tweedie, F. J., & Baayen, R. H. (1998). How variable may a constant be? Measures of lexical richness in perspective. Computers and the Humanities , 32 (5), 323-352.

Van Dongen, N. N., Van Strien, J. W., & Dijkstra, K. (2016). Implicit emotion regulation in the context of viewing artworks: ERP evidence in response to pleasant and unpleasant pictures. Brain and Cognition , 107 , 48-54. doi: 10.1016/j.bandc.2016.06.003

Wang, X., Xie, L., Dong, C., & Shan, Y. (2021). Real-ESRGAN: Training real-world blind super-resolution with pure synthetic data. In 2021 ieee/cvf international conference on computer vision workshops (iccvw) (p. 1905-1914). doi: 10.1109/ICCVW54120.2021.00217

Weiser, M. (1993, 7). Some computer science issues in ubiquitous computing. Commun. ACM , 36 (7), 75-84. doi: 10.1145/159544.159617

White, R. W., Dumais, S. T., & Teevan, J. (2009). Characterizing the influence of domain expertise on web search behavior. In Proceedings of the second acm international conference on web search and data mining (p. 132-141). New York, NY, USA: Association for Computing Machinery. doi: 10.1145/1498759.1498819

Wood, E., Pasquale, D. D., Mueller, J. L., Archer, K., Zivcakova, L., Walkey, K., & Willoughby, T. (2016). Exploration of the relative contributions of domain knowledge and search expertise for conducting internet searches. The Reference Librarian , 57 (3), 182-204. doi: 10.1080/02763877.2015.1122559

Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., . . . Wang, C. (2024). AutoGen: Enabling next-gen LLM applications via multi-agent conversation. Retrieved from https://openreview.net/forum?id= tEAF9LBdgu

Xie, Y., Pan, Z., Ma, J., Jie, L., & Mei, Q. (2023). A prompt log analysis of text-to-image generation systems. In Proceedings of the acm web conference 2023 (p. 3892-3902). New York, NY, USA: Association for Computing Machinery. doi: 10.1145/3543507.3587430

Zenker, F., & Kyle, K. (2021). Investigating minimum text lengths for lexical diversity indices. Assessing Writing , 47 . doi: 10.1016/j.asw.2020.100505

Zhang, L., Rao, A., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In 2023 ieee/cvf international conference on computer vision (iccv) (p. 3813-3824). doi: 10.1109/ ICCV51070.2023.00355

Zhu, W., Huang, L., Zhou, X., Li, X., Shi, G., Ying, J., & Wang, C. (2024). Could AI ethical anxiety, perceived ethical risks and ethical awareness about ai influence university students' use of generative AI products? an ethical perspective. International Journal of Human-Computer Interaction , 0 (0),

1-23. doi: 10.1080/10447318.2024.2323277

Zylinska, J. (2020). AI art: Machine visions and warped dreams . London, UK: Open Humanities Press.

## A Set of images used in Study 1

## A.1 Images with High Aesthetic Appeal

<!-- image -->

<!-- image -->

H1: the foundations of origin, matte painting, genesis, trending on artstation, high resolution

<!-- image -->

H4: eclectic interior of the mind

H5: , ., ., matte painting, 8k cgsociety

H6: The Dude by Glenn Fabry

<!-- image -->

H2: vikings. by Dan Mumford, matte painting, Studio Ghibli

<!-- image -->

<!-- image -->

H8: a moment of silence for our fallen heroes. War memorial. central. CGSociety, painting, postprocessing

<!-- image -->

H3: buck, Hudson River School

H7: fantastic wardrobe of the inner sanctuary comes to life in giant birtation of the soul

H9: tidal wave, matte painting, rendered in octane, ghibli, 8k #epic #wow trending on wikiartH10: portrait of a world war soldier on artstation

<!-- image -->

<!-- image -->

<!-- image -->

## A.2 Images with Low Aesthetic Appeal

<!-- image -->

L1: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL

<!-- image -->

L5: Office Space, Bill Lumbergh. 'yeah, we need you to come in on Saturday, mkay?'

<!-- image -->

<!-- image -->

L9: China buys Russia

<!-- image -->

L10: artwork, academic paper

L3: Asterix at the Robot Games. by Rene Goscinny and Albert UderzoL7: we can do it! propaganda poster

<!-- image -->

L8: My New Band Is Called SyskillL2: a tweet about bias

<!-- image -->

L6: Blind No. 20, Seventeen-foot high Ceiling or Lower, Historical Veridian Green, Indian Yellow Hue, Hansa Yellow Medium (to Mike Kelley)

<!-- image -->

L4: amazing green screen effect

<!-- image -->

<!-- image -->